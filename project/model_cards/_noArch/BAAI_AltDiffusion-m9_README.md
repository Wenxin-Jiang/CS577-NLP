---
language: zh
license: creativeml-openrail-m

tags:
- stable-diffusion
- stable-diffusion-diffusers
- text-to-image
- multilingual
- English(En)
- Chinese(Zh)
- Spanish(Es)
- French(Fr)
- Russian(Ru)
- Japanese(Ja)
- Korean(Ko)
- Arabic(Ar)
- Italian(It)
- diffusers

extra_gated_prompt: |-
  This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
  The CreativeML OpenRAIL License specifies: 
  1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content 
  2. The authors claim no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license
  3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)
  Please read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license
      
extra_gated_heading: Please read the LICENSE to access this model
---

# AltDiffusion

|  åç§° Name   | ä»»åŠ¡ Task       |   è¯­è¨€ Language(s)    | æ¨¡å‹ Model    | Github |
|:----------:| :----:  |:-------------------:| :----:  |:------:|
| AltDiffusion-m9 | å¤šæ¨¡æ€ Multimodal | Multilingual | Stable Diffusion |   [FlagAI](https://github.com/FlagAI-Open/FlagAI)   |

# Gradio

We support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run AltDiffusion-m9:
[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/AltDiffusion-m9)

#  æ¨¡å‹ä¿¡æ¯ Model Information

æˆ‘ä»¬ä½¿ç”¨ [AltCLIP-m9](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltCLIP/README.md)ï¼ŒåŸºäº [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion) è®­ç»ƒäº†åŒè¯­Diffusionæ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®æ¥è‡ª [WuDaoæ•°æ®é›†](https://data.baai.ac.cn/details/WuDaoCorporaText) å’Œ [LAION](https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6plus) ã€‚

æˆ‘ä»¬çš„ç‰ˆæœ¬åœ¨å¤šè¯­è¨€å¯¹é½æ–¹é¢è¡¨ç°éå¸¸å‡ºè‰²ï¼Œæ˜¯ç›®å‰å¸‚é¢ä¸Šå¼€æºçš„æœ€å¼ºå¤šè¯­è¨€ç‰ˆæœ¬ï¼Œä¿ç•™äº†åŸç‰ˆstable diffusionçš„å¤§éƒ¨åˆ†èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æŸäº›ä¾‹å­ä¸Šæ¯”æœ‰ç€æ¯”åŸç‰ˆæ¨¡å‹æ›´å‡ºè‰²çš„èƒ½åŠ›ã€‚

AltDiffusion-m9 æ¨¡å‹ç”±åä¸º AltCLIP-m9 çš„å¤šè¯­ CLIP æ¨¡å‹æ”¯æŒï¼Œè¯¥æ¨¡å‹ä¹Ÿå¯åœ¨æœ¬é¡¹ç›®ä¸­è®¿é—®ã€‚æ‚¨å¯ä»¥é˜…è¯» [æ­¤æ•™ç¨‹](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltCLIP/README.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

We used [AltCLIP-m9](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltCLIP/README.md), and trained a bilingual Diffusion model based on [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion), with training data from [WuDao dataset](https://data.baai.ac.cn/details/WuDaoCorporaText) and [LAION](https://huggingface.co/datasets/laion/laion2B-en).

Our model performs well in aligning multilanguage and is the strongest open-source version on the market today, retaining most of the stable diffusion capabilities of the original, and in some cases even better than the original model.

AltDiffusion-m9 model is backed by a multilingual CLIP model named AltCLIP-m9, which is also accessible in FlagAI. You can read [this tutorial](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltCLIP/README.md) for more information. 


## å¼•ç”¨
å…³äºAltCLIP-m9ï¼Œæˆ‘ä»¬å·²ç»æ¨å‡ºäº†ç›¸å…³æŠ¥å‘Šï¼Œæœ‰æ›´å¤šç»†èŠ‚å¯ä»¥æŸ¥é˜…ï¼Œå¦‚å¯¹æ‚¨çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚

If you find this work helpful, please consider to cite
```
@article{https://doi.org/10.48550/arxiv.2211.06679,
  doi = {10.48550/ARXIV.2211.06679},
  url = {https://arxiv.org/abs/2211.06679},
  author = {Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  title = {AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
```

# æ¨¡å‹æƒé‡ Model Weights

ç¬¬ä¸€æ¬¡è¿è¡ŒAltDiffusion-m9æ¨¡å‹æ—¶ä¼šè‡ªåŠ¨ä»huggingfaceä¸‹è½½å¦‚ä¸‹æƒé‡,  

The following weights are automatically downloaded from HF when the AltDiffusion-m9 model is run for the first time: 

| æ¨¡å‹åç§° Model name              | å¤§å° Size | æè¿° Description                                        |
|------------------------------|---------|-------------------------------------------------------|
| StableDiffusionSafetyChecker | 1.13G   | å›¾ç‰‡çš„å®‰å…¨æ£€æŸ¥å™¨ï¼›Safety checker for image                     |
| AltDiffusion-m9                 | 8.0G    |  support English(En), Chinese(Zh), Spanish(Es), French(Fr), Russian(Ru), Japanese(Ja), Korean(Ko), Arabic(Ar) and Italian(It) |
| AltCLIP-m9                      | 3.22G   | support English(En), Chinese(Zh), Spanish(Es), French(Fr), Russian(Ru), Japanese(Ja), Korean(Ko), Arabic(Ar) and Italian(It)           |


# ç¤ºä¾‹ Example

##  ğŸ§¨Diffusers Example

**AltDiffusion-m9** å·²è¢«æ·»åŠ åˆ° ğŸ§¨Diffusers! 

æˆ‘ä»¬çš„[ä»£ç ç¤ºä¾‹](https://colab.research.google.com/drive/1htPovT5YNutl2i31mIYrOzlIgGLm06IX#scrollTo=1TrIQp9N1Bnm)å·²æ”¾åˆ°colabä¸Šï¼Œæ¬¢è¿ä½¿ç”¨ã€‚

æ‚¨å¯ä»¥åœ¨ [æ­¤å¤„](https://huggingface.co/docs/diffusers/main/en/api/pipelines/alt_diffusion) æŸ¥çœ‹æ–‡æ¡£é¡µé¢ã€‚

ä»¥ä¸‹ç¤ºä¾‹å°†ä½¿ç”¨fast DPM è°ƒåº¦ç¨‹åºç”Ÿæˆå›¾åƒ,  åœ¨V100 ä¸Šè€—æ—¶å¤§çº¦ä¸º 2 ç§’ã€‚

You can run our diffusers example through [here](https://colab.research.google.com/drive/1htPovT5YNutl2i31mIYrOzlIgGLm06IX#scrollTo=1TrIQp9N1Bnm) in colab.

You can see the documentation page [here](https://huggingface.co/docs/diffusers/main/en/api/pipelines/alt_diffusion).

The following example will use the fast DPM scheduler to generate an image in ca. 2 seconds on a V100.

First you should install diffusers main branch and some dependencies:
```
pip install git+https://github.com/huggingface/diffusers.git torch transformers accelerate sentencepiece
```

then you can run the following example:

```python
from diffusers import AltDiffusionPipeline, DPMSolverMultistepScheduler
import torch

pipe = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion-m9", torch_dtype=torch.float16, revision="fp16")
pipe = pipe.to("cuda")

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

prompt = "é»‘æš—ç²¾çµå…¬ä¸»ï¼Œéå¸¸è¯¦ç»†ï¼Œå¹»æƒ³ï¼Œéå¸¸è¯¦ç»†ï¼Œæ•°å­—ç»˜ç”»ï¼Œæ¦‚å¿µè‰ºæœ¯ï¼Œæ•é”çš„ç„¦ç‚¹ï¼Œæ’å›¾"
# or in English:
# prompt = "dark elf princess, highly detailed, d & d, fantasy, highly detailed, digital painting, trending on artstation, concept art, sharp focus, illustration, art by artgerm and greg rutkowski and fuji choko and viktoria gavrilenko and hoang lap"

image = pipe(prompt, num_inference_steps=25).images[0]
image.save("./alt.png")
```

![alt](https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/hub/alt.png)

## Transformers Example

```python
import os
import torch
import transformers
from transformers import BertPreTrainedModel
from transformers.models.clip.modeling_clip import CLIPPreTrainedModel
from transformers.models.xlm_roberta.tokenization_xlm_roberta import XLMRobertaTokenizer
from diffusers.schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler
from diffusers import StableDiffusionPipeline
from transformers import BertPreTrainedModel,BertModel,BertConfig
import torch.nn as nn
import torch
from transformers.models.xlm_roberta.configuration_xlm_roberta import XLMRobertaConfig
from transformers import XLMRobertaModel
from transformers.activations import ACT2FN
from typing import Optional


class RobertaSeriesConfig(XLMRobertaConfig):
    def __init__(self, pad_token_id=1, bos_token_id=0, eos_token_id=2,project_dim=768,pooler_fn='cls',learn_encoder=False, **kwargs):
        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)
        self.project_dim = project_dim
        self.pooler_fn = pooler_fn
        # self.learn_encoder = learn_encoder

class RobertaSeriesModelWithTransformation(BertPreTrainedModel):
    _keys_to_ignore_on_load_unexpected = [r"pooler"]
    _keys_to_ignore_on_load_missing = [r"position_ids", r"predictions.decoder.bias"]
    base_model_prefix = 'roberta'
    config_class= XLMRobertaConfig
    def __init__(self, config):
        super().__init__(config)
        self.roberta = XLMRobertaModel(config)
        self.transformation = nn.Linear(config.hidden_size, config.project_dim)
        self.post_init()
        
    def get_text_embeds(self,bert_embeds,clip_embeds):
        return self.merge_head(torch.cat((bert_embeds,clip_embeds)))

    def set_tokenizer(self, tokenizer):
        self.tokenizer = tokenizer

    def forward(self, input_ids: Optional[torch.Tensor] = None) :
        attention_mask = (input_ids != self.tokenizer.pad_token_id).to(torch.int64)
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        
        projection_state = self.transformation(outputs.last_hidden_state)
        
        return (projection_state,)

model_path_encoder = "BAAI/RobertaSeriesModelWithTransformation"
model_path_diffusion = "BAAI/AltDiffusion-m9"
device = "cuda"

seed = 12345
tokenizer = XLMRobertaTokenizer.from_pretrained(model_path_encoder, use_auth_token=True)
tokenizer.model_max_length = 77

text_encoder = RobertaSeriesModelWithTransformation.from_pretrained(model_path_encoder, use_auth_token=True)
text_encoder.set_tokenizer(tokenizer)
print("text encode loaded")
pipe = StableDiffusionPipeline.from_pretrained(model_path_diffusion,
                                               tokenizer=tokenizer,
                                               text_encoder=text_encoder,
                                               use_auth_token=True,
                                               )
print("diffusion pipeline loaded")
pipe = pipe.to(device)

prompt = "Thirty years old lee evans as a sad 19th century postman. detailed, soft focus, candle light, interesting lights, realistic, oil canvas, character concept art by munkÃ¡csy mihÃ¡ly, csÃ³k istvÃ¡n, john everett millais, henry meynell rheam, and da vinci"
with torch.no_grad():
    image = pipe(prompt, guidance_scale=7.5).images[0]  
    
image.save("3.png")
```


æ‚¨å¯ä»¥åœ¨`predict_generate_images`å‡½æ•°é‡Œé€šè¿‡æ”¹å˜å‚æ•°æ¥è°ƒæ•´è®¾ç½®ï¼Œå…·ä½“ä¿¡æ¯å¦‚ä¸‹:

More parameters of predict_generate_images for you to adjust for `predict_generate_images` are listed below:


| å‚æ•°å Parameter             | ç±»å‹ Type | æè¿° Description                                        |
|--------------------------------|------------|-------------------------------------------------------|
| prompt | str   | æç¤ºæ–‡æœ¬; The prompt text                    |
| out_path | str   | è¾“å‡ºè·¯å¾„; The output path to save images                  |
| n_samples | int   | è¾“å‡ºå›¾ç‰‡æ•°é‡; Number of images to be generate                   |
| skip_grid | bool   | å¦‚æœä¸ºTrue, ä¼šå°†æ‰€æœ‰å›¾ç‰‡æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œè¾“å‡ºä¸€å¼ æ–°çš„å›¾ç‰‡; If set to true, image gridding step will be skipped                    |
| ddim_step | int   | DDIMæ¨¡å‹çš„æ­¥æ•°; Number of steps in ddim model                    |
| plms | bool  | å¦‚æœä¸ºTrue, åˆ™ä¼šä½¿ç”¨plmsæ¨¡å‹; If set to true, PLMS Sampler instead of DDIM Sampler will be applied                    |
| scale | float   | è¿™ä¸ªå€¼å†³å®šäº†æ–‡æœ¬åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå½±å“ç”Ÿæˆçš„å›¾ç‰‡ï¼Œå€¼è¶Šå¤§å½±å“åŠ›è¶Šå¼º; This value determines how important the prompt incluences generate images                    |
| H | int   | å›¾ç‰‡çš„é«˜åº¦; Height of image                    |
| W | int   | å›¾ç‰‡çš„å®½åº¦; Width of image                    |
| C | int   | å›¾ç‰‡çš„channelæ•°; Numeber of channels of generated images                    |
| seed | int   | éšæœºç§å­; Random seed number                     |

æ³¨æ„ï¼šæ¨¡å‹æ¨ç†è¦æ±‚ä¸€å¼ è‡³å°‘10Gä»¥ä¸Šçš„GPUã€‚

Note that the model inference requires a GPU of at least 10G above.


# æ›´å¤šç”Ÿæˆç»“æœ More Results
## multilanguage examples
åŒä¸€å¥promptsä¸åŒè¯­è¨€ç”Ÿæˆçš„äººè„¸ä¸ä¸€æ ·ï¼

One prompts in different languages generates different faces!
![image](./m9.png)
## ä¸­è‹±æ–‡å¯¹é½èƒ½åŠ› Chinese and English alignment ability

### prompt:dark elf princess, highly detailed, d & d, fantasy, highly detailed, digital painting, trending on artstation, concept art, sharp focus, illustration, art by artgerm and greg rutkowski and fuji choko and viktoria gavrilenko and hoang lap
### è‹±æ–‡ç”Ÿæˆç»“æœ/Generated results from English prompts

![image](https://raw.githubusercontent.com/BAAI-OpenPlatform/test_open/main/en_dark_elf.png)

### prompt:é»‘æš—ç²¾çµå…¬ä¸»ï¼Œéå¸¸è¯¦ç»†ï¼Œå¹»æƒ³ï¼Œéå¸¸è¯¦ç»†ï¼Œæ•°å­—ç»˜ç”»ï¼Œæ¦‚å¿µè‰ºæœ¯ï¼Œæ•é”çš„ç„¦ç‚¹ï¼Œæ’å›¾
### ä¸­æ–‡ç”Ÿæˆç»“æœ/Generated results from Chinese prompts
![image](https://raw.githubusercontent.com/BAAI-OpenPlatform/test_open/main/cn_dark_elf.png)

## ä¸­æ–‡è¡¨ç°èƒ½åŠ›/The performance for Chinese prompts

## prompt:å¸¦å¢¨é•œçš„ç”·å­©è‚–åƒï¼Œå……æ»¡ç»†èŠ‚ï¼Œ8Ké«˜æ¸…
![image](https://raw.githubusercontent.com/BAAI-OpenPlatform/test_open/main/boy.png)


## prompt:å¸¦å¢¨é•œçš„ä¸­å›½ç”·å­©è‚–åƒï¼Œå……æ»¡ç»†èŠ‚ï¼Œ8Ké«˜æ¸…
![image](https://raw.githubusercontent.com/BAAI-OpenPlatform/test_open/main/cn_boy.png)

## é•¿å›¾ç”Ÿæˆèƒ½åŠ›/The ability to generate long images

### prompt: ä¸€åªå¸¦ç€å¸½å­çš„å°ç‹— 
### åŸç‰ˆ stable diffusionï¼š
![image](https://raw.githubusercontent.com/BAAI-OpenPlatform/test_open/main/dog_other.png)

### Ours:
![image](https://raw.githubusercontent.com/BAAI-OpenPlatform/test_open/main/dog.png)

æ³¨: æ­¤å¤„é•¿å›¾ç”ŸæˆæŠ€æœ¯ç”±å³è„‘ç§‘æŠ€(RightBrain AI)æä¾›ã€‚

Note: The long image generation technology here is provided by Right Brain Technology.

# è®¸å¯/License

è¯¥æ¨¡å‹é€šè¿‡ [CreativeML Open RAIL-M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) è·å¾—è®¸å¯ã€‚ä½œè€…å¯¹æ‚¨ç”Ÿæˆçš„è¾“å‡ºä¸ä¸»å¼ ä»»ä½•æƒåˆ©ï¼Œæ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨å®ƒä»¬å¹¶å¯¹å®ƒä»¬çš„ä½¿ç”¨è´Ÿè´£ï¼Œä¸å¾—è¿åæœ¬è®¸å¯ä¸­çš„è§„å®šã€‚è¯¥è®¸å¯è¯ç¦æ­¢æ‚¨åˆ†äº«ä»»ä½•è¿åä»»ä½•æ³•å¾‹ã€å¯¹ä»–äººé€ æˆä¼¤å®³ã€ä¼ æ’­ä»»ä½•å¯èƒ½é€ æˆä¼¤å®³çš„ä¸ªäººä¿¡æ¯ã€ä¼ æ’­é”™è¯¯ä¿¡æ¯å’Œé’ˆå¯¹å¼±åŠ¿ç¾¤ä½“çš„ä»»ä½•å†…å®¹ã€‚æ‚¨å¯ä»¥å‡ºäºå•†ä¸šç›®çš„ä¿®æ”¹å’Œä½¿ç”¨æ¨¡å‹ï¼Œä½†å¿…é¡»åŒ…å«ç›¸åŒä½¿ç”¨é™åˆ¶çš„å‰¯æœ¬ã€‚æœ‰å…³é™åˆ¶çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·[é˜…è¯»è®¸å¯è¯](https://huggingface.co/spaces/CompVis/stable-diffusion-license) ã€‚

The model is licensed with a [CreativeML Open RAIL-M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license). The authors claim no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in this license. The license forbids you from sharing any content that violates any laws, produce any harm to a person, disseminate any personal information that would be meant for harm, spread misinformation and target vulnerable groups. You can modify and use the model for commercial purposes, but a copy of the same use restrictions must be included. For the full list of restrictions please [read the license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) .